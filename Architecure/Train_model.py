# -*- coding: utf-8 -*-
"""finaloffinal_project15

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x18krnx2zXbmjAsj_f3_tQZ5c4Q_ZJls
"""

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F
from torchvision.transforms import Compose 
import matplotlib.pyplot as plt
torch.cuda.empty_cache()

import numpy
import torchvision.datasets as datasets
from torchvision.transforms import transforms
cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())

imgs = [item[0] for item in cifar_trainset] # item[0] and item[1] are image and its label
imgs = torch.stack(imgs, dim=0).numpy()

# calculate mean over each channel (r,g,b)
mean_r = imgs[:,0,:,:].mean()
mean_g = imgs[:,1,:,:].mean()
mean_b = imgs[:,2,:,:].mean()
print(mean_r,mean_g,mean_b)

# calculate std over each channel (r,g,b)
std_r = imgs[:,0,:,:].std()
std_g = imgs[:,1,:,:].std()
std_b = imgs[:,2,:,:].std()
print(std_r,std_g,std_b)

from torchvision.transforms import transforms
train_transform = Compose([
    transforms.RandomHorizontalFlip(p=0.5), #flipping half of the training dataset horizontally
    transforms.RandomCrop(32, padding=4,padding_mode="reflect"), #doing a random crop with padding=4
    transforms.ToTensor(), #converting all input training images to tensors
    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]) #normalizing tensor values
])

test_transform = Compose([
    transforms.ToTensor(), #converting all input test images to tensors
    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]) #normalizing tensor values
])
#setting transform to the transformations defined above 
trainingdata = torchvision.datasets.CIFAR10('./CIFAR10/',train=True,download=True, transform=train_transform) #train=true for training data only
testdata = torchvision.datasets.CIFAR10('./CIFAR10/',train=False,download=True,transform=test_transform)

print("Training size: ",len(trainingdata)) #printing the train size
print("Test size: ",len(testdata)) #printing the test data size

trainDataLoader = torch.utils.data.DataLoader(trainingdata,batch_size=128,shuffle=True) 
testDataLoader = torch.utils.data.DataLoader(testdata,batch_size=128,shuffle=False)

class BasicBlock(nn.Module):   
      def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )

      def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out



class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.linear = nn.Linear(256, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 8)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

def project1_model():
    return ResNet(BasicBlock, [5,4,3])

model = project1_model().cuda()
# model

from torch.optim import optimizer
loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01,momentum = 0.9)
#ptimizer=torch.optim.Adam(model.parameters())

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(count_parameters(model))

train_loss_hist  = []
test_loss_hist = []
train_acc_history = [] 
test_acc_history = [] 


for epoch in range(10):
  train_loss =  0.0
  test_loss = 0.0
  correct_points_train = 0 
  correct_points_test = 0 



  for i,data in enumerate(trainDataLoader):
    images,labels = data
    images = images.cuda()
    labels = labels.cuda()
    optimizer.zero_grad()
    y_pred = model(images)
    fit = loss(y_pred, labels)
    fit.backward()
    optimizer.step()
    train_loss += fit.item()
    pred = y_pred.argmax(dim=1, keepdim=True) # get the index of the max log-probability
    correct_points_train += pred.eq(labels.view_as(pred)).sum().item()
 
  for i,data in enumerate(testDataLoader):
    with torch.no_grad():
      images, labels = data
      images = images.cuda()
      labels = labels.cuda()

      y_pred = model(images)
      fit = loss(y_pred,labels)
      test_loss += fit.item()
      pred = y_pred.argmax(dim=1, keepdim=True) # get the index of the max log-probability
      correct_points_test += pred.eq(labels.view_as(pred)).sum().item()

  print(len(trainDataLoader))
  train_loss = train_loss/len(trainDataLoader)
  train_acc = correct_points_train/len(trainDataLoader.dataset) 
  test_loss = test_loss/len(testDataLoader)
  test_acc = correct_points_test/len(testDataLoader.dataset)
  train_loss_hist.append(train_loss)
  test_loss_hist.append(test_loss)
  train_acc_history.append(train_acc) 
  test_acc_history.append(test_acc)

  print('Epoch %s, Train loss %s, Test loss %s'%(epoch, train_loss, test_loss))
  print(f"Train Accuracy: {correct_points_train}/{len(trainDataLoader.dataset)}, {train_acc*100}")
  print(f"Test Accuracy: {correct_points_test}/{len(testDataLoader.dataset)}, {test_acc*100}")

print(train_acc_history)

print(test_acc_history)

print(train_loss_hist)

print(test_loss_hist)

